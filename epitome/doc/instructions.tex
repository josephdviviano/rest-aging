%% EPItome-xl User Manual, Viviano, Joseph David Apr. 2014
\documentclass[final,titlepage,letterpaper,oneside,12pt]{article}
\usepackage[usenames,dvipsnames]{xcolor} % colors
\usepackage{tikz}                       % graphs
\usepackage{caption}                    % figure captions
\usepackage[utf8]{inputenc}             % utf8 encoding compatibility
\usepackage{fourier}                    % fonts
\usepackage[T1]{fontenc}                % fonts
\DeclareFontFamily{OT1}{cmtt}{\hyphenchar \font=-1}
\usepackage{natbib}                     % bibliography
\usepackage{dirtree}                    % folder structures
\usepackage{listings}                   % code blocks
\usepackage[colorlinks=true, pdfstartview=FitV, linkcolor=MidnightBlue, citecolor=MidnightBlue,urlcolor=MidnightBlue]{hyperref} % custom colors
\usetikzlibrary{arrows}                 % arrows
\usepackage{amsmath}                    % equations
\usepackage{epigraph}                   % quotation

\usepackage{listings}

% some code highlighing stuff
\DeclareFontFamily{\encodingdefault}{\ttdefault}{\hyphenchar\font=`\- }
\renewcommand{\texttt}[2][BrickRed]{\textcolor{#1}{\ttfamily #2}}% \texttt[<color>]{<stuff>}

\newcommand{\atilde}{\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}
\renewcommand\epigraphflush{flushright} % 
\renewcommand\epigraphsize{\normalsize} % 
\setlength\epigraphwidth{0.7\textwidth} % 
\definecolor{titlepagecolor}{cmyk}{1,.60,0,.40} % custom color

\newenvironment{blockquote}{%
  \par%
  \medskip
  \leftskip=4em\rightskip=2em%
  \noindent\ignorespaces}{%
  \par\medskip}

%% Title Page
\makeatletter                       
\def\printauthor{%                  
    {\normalsize \@author}}              
\makeatother
\author{%
    Joseph D. Viviano \\
    josephdviviano@gmail.com \\
    York University, TO, CA. \\
    \today
}

\newcommand\titlepagedecoration{%
\begin{tikzpicture}[remember picture,overlay,shorten >= -10pt]

\coordinate (aux1) at ([yshift=-15pt]current page.north east);
\coordinate (aux2) at ([yshift=-410pt]current page.north east);
\coordinate (aux3) at ([xshift=-4.5cm]current page.north east);
\coordinate (aux4) at ([yshift=-150pt]current page.north east);

\end{tikzpicture}%
}

\begin{document}
\begin{titlepage}

\noindent
\begin{center}
\begin{Huge}
EPItome-xl\par
\end{Huge}
\end{center}
\vspace*{2cm}
\epigraph{As long as our brain is a mystery, the universe, the reflection of the structure of the brain will also be a mystery.}%
{\textsc{Santiago Ram√≥n y Cajal}}
\null\vfill
\vspace*{1cm}
\noindent
\hfill
\begin{minipage}{0.55\linewidth}
    \begin{flushright}
        \printauthor
    \end{flushright}
\end{minipage}
%
\begin{minipage}{0.02\linewidth}
    \rule{1pt}{60pt}
\end{minipage}
\titlepagedecoration
\end{titlepage}

% Table of contents
\tableofcontents
\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Overview of the Pipeline
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

EPItome-xl is a program designed for the flexible construction of MRI pre-processing pipelines, with a focus on functional MRI images and their associated problems. Its primary function is to take BASH modules and chain them together in any way the user desires to create a set of batch-processing scripts for an MRI experiment. These modules are not necessarily dependent on one another, allowing users of this package to easily extend the functionality of EPItome-xl by simply depositing a shell script into the appropriate module folder and writing the associated python wrapper command (and documentation!)

The goal of this design is to allow multiple levels of control for users of different skills with the same interface. EPItome facilitates the construction of very robust pre-processing scripts that can be run on your computer or in a distributed computing environment by only answering a few high-level questions, hopefully making it easy for beginners to get started. The scripts that are output by these commands are otherwise fully tweak-able and well commented -- they encourage experimentation. These modified modules could eventually evolve into new features altogether, which are easily added to the existing pool.

This system is also designed to facilitate easy-to-reproduce research, as these scripts can be easily re-purposed for new experiments that follow the EPItome folder structure. In this way, the outputs of EPItome act as your lab notebook, and can be shared with collaborators or reviewers.

This manual will progress to more advanced topics in the end. First, I will explain the basic use of EPItome-xl. Next, I'll explain the modules one-by-one, follow with a description of a few common pre-processing tasks. I'll then  and finish with an explanation on how to add new modules.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Requirements
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Requirements \& Dependencies}

EPItome contains a small number of programs that actually manipulate data, but also makes heavy use of widely-used MRI analysis tools and a number of python distributions. The user is assumed to have properly installed and configured FSL, AFNI, Freesurfer, and the python packages numpy, scipy, and matplotlib. For physiological noise regression, you must have the MATLAB compiler runtime installed, along with AFNI's \href{http://afni.nimh.nih.gov/sscc/dglen/McRetroTS}{McRetroTS} scripts installed in \texttt{/opt/MATLAB/MATLAB\_Compiler\_Runtime/} and \texttt{/opt/mcretro/}, respectively.

The program itself was built and tested on a Ubuntu 12.04 server. I imagine it will work well in any Linux environment. It should run on Mac OS X as well, but this remains unverified. There will be no support for Windows.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The functionality of the pipeline
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Setting Up \& Using EPItome-xl}

Up to date EPItome code \& installation instructions are available from \href{https://github.com/josephdviviano/EPItome-xl}{github}). Briefly, \\

% code to configure EPItome-xl
\begin{lstlisting}[language=bash,frame=L,basicstyle=\footnotesize\ttfamily]
git clone https://github.com/josephdviviano/EPItome-xl.git
easy_install EPItome-xl
\end{lstlisting}
\

After installation, EPItome requires you to specify a few paths. These settings can be found in the /epitome/config.py file.

\begin{itemize}
    \item{\texttt{dir\_data}: should point to your MRI experiment folder.}
    \item{\texttt{dir\_pipe}: should point to your installation of EPItome-xl.}
    \item{\texttt{dir\_afni}: should point to your installation of AFNI.}
    \item{\texttt{cores}: could be set to the maximum number of cores you wish EPItome to use (defaults to the maximum possible -1).}
\end{itemize}

EPItome-xl comes with a few command-line interfaces. \texttt{EPItome} is used to inspect data in the MRI directory, returns information on the currently-available modules, can be used to construct new pipelines, and to remove unwanted data from the MRI directory cleanly. \texttt{EPIphysio} is a tool built to parse physiological data from the BIOPAK 150 unit we have installed at York University (Toronto), and might need to be adapted / extended to work with other units. \texttt{EPIfolder} is used to generate an appropriate folder structure in the MRI directory for the EPItome pipeline to work on.

The MRI directory itself must be organized as follows: \\

%framebox hack fixes indentation problems
\makebox{\begin{minipage}[t]{1\columnwidth}%
\dirtree{%
    .1 /EPItome.
    .2 EXPERIMENTS.
    .3 SUBJECTS.
    .4 MODE.
    .5 SESS.
    .6 RUN.
    .2 FREESURFER.
    .3 SUBJECTS.
}
\
\end{minipage}}

\noindent
The Freesurfer subject directory should point to the Freesurfer directory in your MRI folder -- this can easily be accomplished using a \href{https://kb.iu.edu/d/abbe}{symbolic link}. The remaining folder structure is perhaps best generated by using the included \texttt{EPIfolder} tool (or via a home-brew script).

\subsection{Folders}

The folder structure is integral to EPItome -- if it is flawed, the pipeline will fail in \href{https://www.youtube.com/watch?v=TxcDTUMLQJI}{mysterious ways}. The structure itself is designed to be thought of as a tree. At the roots of the tree are the individual files collected at the scanner. As we ascend the tree, files are combined across sessions, image modalities, and subjects, so one finds experiment-wide outputs at the highest levels. The \texttt{EPIfolder} program will help you set up these folders appropriately.

\subsubsection{EXPERIMENTS}

This is a set of folders containing entire experiments. There are no important naming conventions, but it seems advisable (for consistency) to make the folder names all capitals, and short (e.g., \texttt{LINGASD} for `language study on those with autism spectrum disorder').

\subsubsection{SUBJECTS}

Once again, these are simply folders with participant names. They follow no convention, but should be consistent for your own sake.

\subsubsection{MODE}

Image modality folders separate images of different kinds: anatomicals, EPI's collected using differing sequences, or EPI's of different task-types (e.g., rest vs. two-back matching). The T1 directory \textbf{must} exist for each subject at the very minimum in \texttt{SESS01}.

This is a good place to separate scans you would like to have analyzed in different ways, or to test multiple pre-processing strategies on the same set of subjects. For example, it may be that your \texttt{TASK} set is being prepared for a GLM or partial least squares analysis, and should be processed more minimally than your \texttt{REST} data, which will undergo things such as low-pass filtering and nuisance variable regression. In another example, it may be that you are curious about how your choice of pre-processing steps influences your results. Here, you could have a set of identical scans under \texttt{REST\_1} and \texttt{REST\_2}. You could build two sets of pipelines using EPItome with the unique identifiers `1' and `2', and run them on each modality separately.

EPItome has no modules built for DTI scans at the moment, but they could easily be added here under their own DTI modality. Note that a set of DTI-friendly modules would need to be built for these kinds of scans.

\subsubsection{SESS}

The session folders are used to separate scans taken on different days. They must begin with \texttt{SESS} and end with a zero-padded 2-digit number (e.g., \texttt{02} or \texttt{10}). EPItome does not currently support experiments where participants were scanned on more than 99 days.

These session folders are currently used to match EPIs with the T1 taken on the same day. Best practice is to collect a T1 with every EPI scan. The pipeline is also able to use the T1 collected on the first day as the target for all sessions. This will be automatically decided by the pipeline: if the number of T1s does not equal the number of sessions, only the first T1 will be used.

\textbf{IMPORTANT:} While it is normally advisable for the sessions to align chronologically, in the case that the only T1 collected was \textit{not} on the first day, it should still be entered as \texttt{SESS01}.

\subsubsection{RUN}

Each \texttt{RUN} folder should contain one and only one \texttt{.nii} or \texttt{.nii.gz} formatted file. Appropriate companion files should also be entered here: physiological noise recordings (extension \texttt{.PHYS}, and/or custom slice timing files (extension \texttt{.1D}). If more than NIFTI file is in this folder, the pipeline will fail. Any other files kept in this folder will remain untouched, so this is a fine place to keep run-specific notes.

\subsection{Running EPItome}

\texttt{EPItome} contains a set of helper subroutines and two main functions: `run' and `clean'. Typing \texttt{EPItome} into your command line after installation should show each function and a brief description of each, so I won't reiterate that here. I will mention that \texttt{EPItome check <experiment>} allows you to check the total number of raw NIFTIs in the \texttt{RUN} folders of each image modality. This allows you to quickly find empty \texttt{RUN} folders, and ensure you have properly imported all of your data.

\subsubsection{EPItome run}

This is the heart and soul of \texttt{EPItome}. This command line interface will walk you through the construction of a pipeline for a single image modality within a single experiment. Every run of EPItome begins with a lengthly run of Freesurfer on all T1s, and \texttt{init\_EPI}, which does the most basic kinds of MRI pre-processing. Following that, you are free to chain together modules as you see fit.

In order to retain the modular and easily-customized structure of EPItome, this program allows you to shoot yourself in the foot. In fact, if you aren't clear on what to do, you are more likely to make a malformed pipeline than you are to making a good one. Therefore, I recommend reading the Pipeline section of this manual at least once, and perhaps skimming the Presets section following, to get a sense of reasonable usage.

The pipeline will allow you to chain together various modules in order until you give it the \texttt{stop} command, when it will switch over to asking you to submit a set of desired QC outputs. Internally, this program is simply looking through the \texttt{/qc} module directory instead of the \texttt{/pre} module directory. These QC outputs will only properly work once all of your subjects are pre-processed, as they output single PDFs detailing some feature of the entire experiment.

Finally, a few outputs will be deposited in your experiment directory: a \texttt{master} script, a \texttt{proclist} script, and a set of \texttt{cmd} scripts. A copy of all the current modules at the time of running will be deposited in an \texttt{EPItome} directory in your home folder. The \texttt{master} script generates everything else from these copied modules, and can be edited by-hand to produce new pipelines. In fact, those who know what they are doing can generate new pipelines directly from an old \texttt{master} script, instead of interacting with the command line interface again. The \texttt{cmd} scripts are the actual set of commands run on each participant. These are essentially the module script files concatenated in the way defined by the user with the appropriate variables filled in. These files are meant to be well-commented, and should be easily edited by-hand for those adventurous types who would like to tweak various settings, try new methods, or debug beguiling problems, on a subject wise basis. Alternatively, the modules in your home folder can be edited, the master script re-run, and the changes will be applied to every subject's \texttt{cmd} script. The \texttt{proclist} is simply a set of calls to these various scripts in order. It can be called directly, to run the subjects serially, or submitted to a batch queuing system to be analyzed in a cluster environment.

EPItome scripts are written to never re-do done work. Therefore, to replace a bad set of outputs with good ones, one must first delete the bad outputs. This can be done by hand, or via a set of helper cleanup scripts, detailed below.

\subsubsection{EPItome clean}

This program works similarly to \texttt{run}, but produces scripts for deleting intermediate files. Generally, it is good practice to inspect the outputs of EPItome first, and if problems are identified, to work backwards through the pipeline until the problem arises to determine the origin of the issue. After the outputs have been vetted, these cleanup scripts will go a long way to keeping your hard drives and system administrators happy.

\subsubsection{EPIfolder}

This simple tool will help you generate folders properly-formatted for EPItome. It is run on a per-subject basis, but a clever user could manually duplicate a single folder structure for as many participants as needed. These folders will automatically be generated in the designated working directory.

\subsubsection{EPIphysio}

EPItome will automatically regress physiological noise out of your data, if you place it (appropriately named) in each RUN folder. The BioPak system outputs a single giant set of physiological data for the entirety of an MRI session. This program will take in this single file and split it into a set of heart rate and respiration time series for each run. These output files are placed in your current working directory and will need to be sorted manually.

\subsubsection{EPIqueue}

EPItome eventually generates a `proclist', a set of commands that must be executed in order to generate the desired outputs. This proclist can be run manually, if desired, but can also be submitted to the sun-grid queuing system by using EPIqueue. This is generally preferred, as multiple users attempting to run proclists simultaneously might overload the system.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Pipeline specifics: usage of each module.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Pipeline}

What follows is a description of what each module in EPItome-xl does. These modules can be easily chained together manually, or by using the command-line interface included with the pipeline. New modules are simply bash scripts which call various programs, including FSL, Freesurfer, AFNI, and custom python programs. Therefore, functionality of EPItome is easily extended without perturbing the function of older modules. Any script found in the modules directories will be added to the command line interface automatically, but will not work properly unless a matching wrappper function is added to \texttt{commands.py}.

The types of modules included can be roughly split into 4 categories: freesurfer, pre-processing, quality-control, and cleanup.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Freesurfer}

Right now, the default freesurfer recon-all is run on every participant before further processing. This is to produce surface files that can be used for cortical smoothing / data visualization, and the automatic generation of tissue masks which can be used for the generation of nuisance regressors. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{fsrecon.py}
Usage: \texttt{fsrecon.py <data\_directory> <experiment> <modality> <cores>} \

\begin{blockquote}
data\_directory -- full path to your MRI/WORKING directory. \\
experiment -- name of the experiment being analyzed. \\
modality -- image modality to import (normally T1). \\
cores -- number of cores to dedicate (one core per run). \
\end{blockquote}

\noindent This sends each subject's T1s through the Freesurfer pipeline. It uses multiple T1s per imaging session, but does not combine them between sessions. Data is output to the dedicated FREESURFER directory, and should be exported to the MRI analysis folders using \texttt{fsexport.py}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{fsexport.py}
Usage: \texttt{fsexport.py <data\_directory> <experiment>}

\begin{blockquote}
data\_directory -- full path to your MRI/WORKING directory. \\
experiment -- name of the experiment being analyzed. \
\end{blockquote}

\noindent Imports processed T1s from Freesurfer to the experiment directory.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Pre-Processing}

This contains the lion's share of the pipeline. Every run of EPItome begins with \texttt{init\_EPI}, which contains a non-contentious set of pre-processing steps for EPI images. THe following stages can be chained together at will to preform de-noising, spatial transformations, projections to surface-space, and spatial smoothing.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{init\_EPI}
Usage: \texttt{init\_EPI <data\_quality> <del\_tr> <t\_pattern> <normalization> <masking>}

\begin{blockquote}
data\_quality -- `low' for poor internal contrast, otherwise `high'. \\
del\_tr -- number of TRs to remove from the beginning of the run. \\
t\_pattern -- slice-timing at acquisition (from AFNI's 3dTshift). \
normalization -- voxel wise time series normalization. One of `zscore', `pct', `demean'. \\
masking -- EPI brain masking tolerance. One of `loose', `normal' `tight'. \
\end{blockquote}

\noindent Works from the raw data in each RUN folder. It performs general pre-processing for all fMRI data:

\begin{itemize} \itemsep-2pt
    \item{Orients data to RAI}
    \item{Deletes initial time points (optionally)}
    \item{Removes data outliers}
    \item{Slice time correction}
    \item{Deobliques \& motion corrects data}
    \item{Creates session mean deskulled EPIs and whole-brain masks}
    \item{Scales and optionally normalizes each time series}
    \item{Calculates various statistics + time series}
\end{itemize}

Times series normalization can be accomplished in one of two ways: percent signal change, and scaling. For percent signal change, the data is normalized by the mean of each time series to mean = 100. A deviation of 1 from this mean value indicates 1\% signal change in the data. This is helpful for analyzing only relative fluctuations in the signal and is best at removing inter-session/subject/run variability, although it can also introduce rare artifacts in small localized regions of the images and may not play well with multivariate techniques such as partial least squares without accounting for these artifacts. Alternatively, one can scale the data, which applies single scaling factor to all voxels such that the global mean of the entire run = 1000. This will help normalize baseline shifts across sessions, runs, and participants. Your selection here might be motivated by personal preference, or in rarer cases, analytic requirements. When in doubt, it is safe to select `off', as scaling can be done later by hand, or `scale' if one is doing a simple GLM-style analysis. `pct' should be used by those with a good reason.

Masking options are provided to improve masking performance across various acquisition types, but it is very hard to devise a simple one-size fits all solution for this option. Therefore the QC outputs will be very important for ensuring good masking, and these options may need to be tweaked on a site-by-site basis. Luckily, many analysis methods do not rely heavily on mask accuracy. In cases that do, such as partial least squares / ICA / PCA analysis, close attention should be paid to the output of this step. Hopefully the `loose', `normal', and `tight' nomenclature are self-explanatory. Generally, it is best to start with normal, and adjust if required.

\
Prerequisites: None.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{combine\_volumes}
Usage: \texttt{combine\_volumes <func1\_prefix> <func2\_prefix>}

\begin{blockquote}
func1\_prefix -- functional data prefix (eg., smooth in func\_smooth).
func2\_prefix -- functional data prefix (eg., smooth in func\_smooth).
\end{blockquote}

\noindent Combines two functional files via addition. Intended to combine the outputs of \texttt{surfsmooth} \& \texttt{surf2vol} with \texttt{volsmooth}, but could be used to combine other things as well. The functional files should not have non-zeroed regions that overlap, or the output won't make much sense.

Prerequisites: Two EPI modules with unique output prefixes. Intended to be used to combine the outputs of \texttt{volsmooth} and \texttt{surfsmooth} in a single volume. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{concatenate}
Usage: \texttt{concatenate <func\_prefix>}

\begin{blockquote}
func\_prefix -- functional data prefix (eg., smooth in func\_smooth).
\end{blockquote}

\noindent Concatenates all runs in order of a particular type from a particular run of the pipeline (i.e., they will only be drawn from those with the same unique identifier).

Prerequisites: \texttt{init\_EPI}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{ICA}
Usage: \texttt{ICA <func\_prefix> <mask\_prefix>}

\begin{blockquote}
func\_prefix -- functional data prefix (eg., smooth in func\_smooth).
anat\_prefix -- mask data prefix (eg., EPI\_brain in anat\_EPI\_brain).
\end{blockquote}

\noindent Runs ICA on each input functional file of the type defined using the default MELODIC settings. This module could be easily tweaked to grant the user access to the dimensionality estimation settings, if need be. The output is the full MELODIC report in a `.ica' folder.

Prerequisites: \texttt{init\_EPI}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{linreg\_calc\_AFNI}
Usage: \texttt{linreg\_calc\_AFNI <cost> <reg\_dof> <data\_quality>}

\begin{blockquote}
cost -- cost function minimized during registration. \\
reg\_dof -- `big\_move' or `giant\_move' (from align\_EPI\_anat.py). \\
data\_quality -- `low' for poor internal contrast, otherwise `high'. \
\end{blockquote}

\noindent Uses AFNI's align\_EPI\_anat.py to calculate linear registration between EPI <--> T1 <--> MNI152, and generate an EPI template registered to T1 \& T1 registered to EPI (sessionwise). Specific options can be found in the command-line interface's help function.

Prerequisites: \texttt{init\_EPI}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{linreg\_calc\_FSL}
Usage: \texttt{linreg\_calc\_FSL <cost> <reg\_dof> <data\_quality>}

\begin{blockquote}
cost -- cost function minimized during registration (see FSL FLIRT). \\
reg\_dof -- 6, 7, 9, or 12 degrees of freedom (see FSL FLIRT), \\
data\_quality -- `low' for poor internal contrast, otherwise `high'. \
\end{blockquote}

\noindent Uses FSL's FLIRT to calculate linear registration between EPI <--> T1 <--> MNI152, and generate an EPI template registered to T1 \& T1 registered to EPI (sessionwise). Specific options can be found in the command-line interface's help function.

Prerequisites: \texttt{init\_EPI}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{linreg\_EPI2MNI\_AFNI}
Usage: \texttt{linreg\_EPI2MNI\_AFNI <func\_prefix> <voxel\_dims>}

\begin{blockquote}
func\_prefix -- functional data prefix (eg.,smooth in func\_smooth). \\
voxel\_dims -- target voxel dimensions (isotropic). \
\end{blockquote}

\noindent Prepares data for analysis in MNI standard space.

Prerequisites: \texttt{init\_EPI}, \texttt{linreg\_calc\_AFNI}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{linreg\_EPI2MNI\_FSL}
Usage: \texttt{linreg\_EPI2MNI\_FSL <func\_prefix> <voxel\_dims>}

\begin{blockquote}
func\_prefix -- functional data prefix (eg., smooth in func\_smooth). \\
voxel\_dims -- target voxel dimensions (isotropic). \
\end{blockquote}

\noindent Prepares data for analysis in MNI standard space.

Prerequisites: \texttt{init\_EPI}, \texttt{linreg\_calc\_FSL}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{linreg\_FS2EPI\_AFNI}
Usage: \texttt{linreg\_FS2EPI\_AFNI}

\noindent Brings Freesurfer atlases in register with single-subject EPIs.

Prerequisites: \texttt{init\_EPI}, \texttt{linreg\_calc\_AFNI}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{linreg\_FS2EPI\_FSL}
Usage: \texttt{linreg\_FS2EPI\_FSL}

\noindent Brings Freesurfer atlases in register with single-subject EPIs.

Prerequisites: \texttt{init\_EPI}, \texttt{linreg\_calc\_FSL}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{linreg\_FS2MNI\_FSL}
Usage: \texttt{linreg\_FS2MNI\_FSL}

\noindent Brings Freesurfer atlases in register with MNI standard space.

Prerequisites: \texttt{init\_EPI}, \texttt{linreg\_calc\_FSL}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{linreg\_T12MNI\_AFNI}
Usage: \texttt{linreg\_T12MNI\_AFNI}

\noindent Brings T1 data in register with MNI standard space.

Prerequisites: \texttt{init\_EPI}, \texttt{linreg\_calc\_AFNI}, \texttt{linreg\_EPI2T1\_AFNI}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{nonlinreg\_calc\_AFNI}
Usage: \texttt{nonlinreg\_calc\_AFNI}

\noindent Computes a nonlinear warp from linear-registered individual T1 to MNI space. This can be concatenated onto the end of a set of linear warps to push EPI data into a more homogeneous space than by simply nonlinearly-warping the data, but beware the findings one can obtain by forcing a square peg into a round hole! Requires these linear registrations to be completed first.

Prerequisites: \texttt{init\_EPI}, \texttt{linreg\_calc\_AFNI}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{nonlinreg\_EPI2MNI\_AFNI}
Usage: \texttt{nonlinreg\_EPI2MNI\_AFNI <func\_prefix> <voxel\_dims>}

\begin{blockquote}
func\_prefix -- functional data prefix (eg., smooth in func\_smooth). \\
voxel\_dims -- target voxel dimensions (isotropic). \
\end{blockquote}

\noindent Prepares data for analysis in MNI standard-space, including a nonlinear warp.

Prerequisites: \texttt{init\_EPI}, \texttt{linreg\_calc\_AFNI}, \texttt{nonlinreg\_calc\_AFNI}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{gen\_regressors}
Usage: \texttt{gen\_regressors <func\_prefix>}

\begin{blockquote}
func\_prefix -- functional data prefix (eg.,smooth in func\_smooth).
\end{blockquote}

\noindent Creates a series of regressors from fMRI data and a freesurfer segmentation: 

\begin{itemize} \itemsep-2pt
	\item{white matter + eroded mask}
	\item{ventricles + eroded mask}
	\item{grey matter mask}
	\item{brain stem mask}
	\item{dialated whole-brain mask}
	\item{draining vessels mask}
	\item{local white matter regressors + 1 temporal lag}
	\item{ventricle regressors + 1 temporal lag}
	\item{draining vessel regressors + 1 temporal lag}
\end{itemize}

Prerequisites: \texttt{init\_EPI}, \texttt{linreg\_calc\_AFNI/FSL}, \texttt{linreg\_FS2EPI\_AFNI/FSL}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{gen\_gcor}
Usage: \texttt{gen\_gcor <func\_prefix>}

\begin{blockquote}
func\_prefix -- functional data prefix (eg.,smooth in func\_smooth).
\end{blockquote}

\noindent Calls an AFNI script to calculate the global correlation for each concatenated set of runs (across all sessions). Useful for resting state functional connectivity experiments.

Prerequisites: \texttt{init\_EPI}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{filter}
Usage: \texttt{filter <func\_prefix> <det> <gs> <vent> <dv> <wm\_loc> <wm\_glo>}

\begin{blockquote}
func\_prefix -- functional data prefix (eg.,smooth in func\_smooth). \\
det -- polynomial order to detrend each voxel against. \\
gs -- if == on, regress mean global signal from each voxel. \\
vent -- if == on, regress mean ventricle signal from each voxel. \\
dv -- if == on, regress mean draining vessel signal from each voxel. \\
wm\_loc -- if == on, regress local white matter from target voxels. \\
wm\_glo -- if == on, regress global white matter for all voxels. \

\end{blockquote}

\noindent This computes detrended nuisance time series, fits each run with a computed noise model, and subtracts the fit. Computes temporal SNR. This program always regresses the motion parameters \& their first lags, as well as physiological noise regressors generated my McRetroTS if they are available. The rest are optional, and generally advisable save global mean regression.

Prerequisites: \texttt{init\_EPI}, \texttt{linreg\_calc\_AFNI/FSL}, \texttt{linreg\_FS2EPI\_AFNI/FSL}, \texttt{gen\_regressors}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{TRdrop}
Usage: \texttt{TRdrop <func\_prefix> <head\_size> <FD\_thresh> <DV\_thresh>}

\begin{blockquote}
func\_prefix -- functional data prefix (eg.,smooth in func\_smooth). \\
head\_size -- head radius in mm (def. 50 mm). \\
thresh\_FD -- censor TRs with $\Delta$ motion > $x$ mm (def. 0.3 mm). \\
thresh\_DV -- censor TRs with $\Delta$ GS change > $x$ \% (def. \href{http://upload.wikimedia.org/wikipedia/en/1/16/Drevil_million_dollars.jpg}{1000000} \%). \
\end{blockquote}

\noindent This removes motion-corrupted TRs from fMRI scans and outputs shortened versions for connectivity analysis (mostly). By default, DVARS regression is set of OFF by using a very, very high threshold.

Prerequisites: \texttt{init\_EPI}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{lowpass}
Usage: \texttt{lowpass <func\_prefix> <mask\_prefix> <filter> <cutoff>}

\begin{blockquote}
func\_prefix -- functional data prefix (eg.,smooth in func\_smooth). \\
mask\_prefix -- mask data prefix (eg., EPI\_mask in anat\_EPI\_mask). \\
filter -- filter type: `median', `average', `kaiser', or `butterworth'. \\
cutoff -- filter cuttoff: either window length, or cutoff frequency. \
\end{blockquote}

\noindent This low-passes input data using the specified filter type and cutoff. 

Both `median' and `average' filters operate in the time domain and therefore, the best cutoff values are odd (and must be larger than 1 to do anything). Time-domain filters are very good at removing high-frequency noise from the data without introducing any phase-shifts or ringing into the time series. When in doubt, a moving average filter with window length of 3 is a decent and conservative choice.

Alternatively, the `kaiser' and `butterworth' filters work in the frequency domain and accept a cutoff in Hz (people tend to use a default of 0.1). Both are implemented as bi-directional FIR filters. The kaiser window is high order and permits reasonably sharp rolloff with minimal passband ringing for shorter fMRI time series. The butterworth filter is of low order and achieves minimal passband ringing at the expense of passband roll off (in layman's terms, butterworth filters will retain more high-frequency content than a kaiser filter with equivalent cutoff). The effect of the passband ringing is an empirical question that would be best tested by the User.

Prerequisites: \texttt{init\_EPI}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{surfsmooth}
Usage: \texttt{surfsmooth <func\_prefix> <FWHM>}

\begin{blockquote}
func\_prefix -- functional data prefix (eg.,smooth in func\_smooth). \\
FWHM -- full-width half-maximum of the gaussian kernel convolved with the surface data. \
\end{blockquote}

\noindent This spatially-smooths cortical data along the surface mesh, estimated by Freesurfer.

Prerequisites: \texttt{init\_EPI}, \texttt{linreg\_calc\_AFNI}, \texttt{linreg\_EPI2T1\_AFNI}, \texttt{vol2surf}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{surf2vol}
Usage: \texttt{surf2vol <func\_prefix> <target\_prefix>}

\begin{blockquote}
func\_prefix -- functional data prefix (eg.,smooth in func\_smooth). \\
target\_prefix -- target data prefix (eg.,smooth in func\_smooth). \
\end{blockquote}

\noindent This projects surface data back into a functional volume with the same properties as <target\_prefix>.

Prerequisites: \texttt{init\_EPI}, \texttt{linreg\_calc\_AFNI}, \texttt{linreg\_EPI2T1\_AFNI}, \texttt{vol2surf}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{vol2surf}
Usage: \texttt{vol2surf <func\_prefix>}

\begin{blockquote}
func\_prefix -- functional data prefix (eg.,smooth in func\_smooth).
\end{blockquote}

\noindent Projects functional data from volume space to a Freesurfer generated cortical mesh. This must be run on EPI data in single-subject T1 space, otherwise we won't end up projecting the cortex to the surface model, but rather some random selection of brain and non-brain matter!

Prerequisites: \texttt{init\_EPI}, \texttt{linreg\_calc\_AFNI}, \texttt{linreg\_EPI2T1\_AFNI}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{volsmooth}
Usage: \texttt{volsmooth <func\_prefix> <mask\_prefix> <FWHM>}

\begin{blockquote}
func\_prefix -- functional data prefix (eg., smooth in func\_smooth).
mask\_prefix -- mask data prefix (eg., EPI\_mask in anat\_EPI\_mask).
func\_prefix -- functional data prefix (eg.,smooth in func\_smooth).
\end{blockquote}

\noindent Re-samples a mask containing one or more labels to the functional data and smooths within unique values. All zero values in the mask are zeroed out in the output. The output of this can be combined with the outputs of \texttt{surfsmooth} \& \texttt{surf2vol} using \texttt{combine\_volumes}.

Prerequisites: \texttt{init\_EPI}, \texttt{linreg\_calc\_AFNI}, \texttt{linreg\_EPI2T1\_AFNI}, \texttt{vol2surf}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Quality Control}

These programs run experiment-wide, and therefore are run after all \texttt{/pre} modules have completed for every subject. They produce reports that give a broad overview of the data quality at different stages of pre-processing, encouraging visual inspection of the data and hopefully reducing the amount of time spent hunting for the source of bugs when they do arise.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{check\_EPI2T1}
Usage: \texttt{check\_EPI2T1 <path> <expt> <mode>}

\begin{blockquote}
path -- full path to EPItome data folder.
expt -- experiment name.
mode -- image modality (eg., TASK, REST).
\end{blockquote}

\noindent Prints out a PDF showing the quality of the linear registration between the functional and anatomical data for all subjects. On the top row, the EPI image is translucent and overlaid on the anatomical in red. On the bottom row, an edge-detected version of the anatomical is overlain on the EPI image in blues.

Prerequisites: \texttt{linreg\_calc\_AFNI/FSL}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{check\_T12MNI}
Usage: \texttt{check\_T12MNI <path> <expt> <mode>}

\begin{blockquote}
path -- full path to EPItome data folder.
expt -- experiment name.
mode -- image modality (eg., TASK, REST).
\end{blockquote}

\noindent Prints out a PDF showing the quality of the linear registration between the subject-specific anatomical data and the group-level MNI brain. On the top row, the T1 image is translucent and overlaid on the MNI brain in red. On the bottom row, the MNI brain is translucent and is overlaid on the T1 brain in red.

Prerequisites: \texttt{linreg\_calc\_AFNI/FSL}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{check\_masks}
Usage: \texttt{check\_masks <path> <expt> <mode>}

\begin{blockquote}
path -- full path to EPItome data folder.
expt -- experiment name.
mode -- image modality (eg., TASK, REST).
\end{blockquote}

\noindent Prints out a PDF showing the Freesurfer-derived masks overlain on each subject's T1 brain -- these masks are those used for regressor estimation. White matter is labeled in dark blue, gray matter is labeled in light blue, draining vessels are labeled in light blue?, and ventricles are labeled in ???.  

Prerequisites: \texttt{linreg\_FS2EPI\_AFNI/FSL}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{check\_MC\_TRs}
Usage: \texttt{check\_MC\_TRs <path> <expt> <mode>}

\begin{blockquote}
path -- full path to EPItome data folder.
expt -- experiment name.
mode -- image modality (eg., TASK, REST).
\end{blockquote}

\noindent Prints TRs 6-10 from the first run of each session. The default motion-correction TR 8 is marked in red. This will allow the user to identify whether the motion-correction TR is somehow corrupted for any given subject, which can be manually changed and re-run.

Prerequisites: \texttt{init\_EPI}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{check\_motionind}
Usage: \texttt{check\_motionind <path> <expt> <mode> <uid>}

\begin{blockquote}
path -- full path to EPItome data folder.
expt -- experiment name.
mode -- image modality (eg., TASK, REST).
uid -- unique identifier for run of EPItome.
\end{blockquote}

\noindent This prints the estimated framewise displacement and DVARS measurement for all subjects and runs in an experiment to one grid, ranking subjects by the sum of their framewise displacement (top left shows least motion, bottom right shows most motion). Vertical lines denote runs, and the horizontal line denotes a respectable threshold of 0.5 mm/TR for the framewise displacement plot, and 10\% signal change for the DVARS plot. This can be used to hopefully facilitate subject-wise or run-wise rejection due to excessive head motion.

Prerequisites: \texttt{init\_EPI}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{check\_runs}
Usage: \texttt{check\_runs <path> <expt>}

\begin{blockquote}
path -- full path to EPItome data folder.
expt -- experiment name.
\end{blockquote}

\noindent This prints out a CSV containing the NIFTI dimensions of each file contained in a RUN folder. This works across all modalities simultaneously, and records subject, image modality, session, and run number. This should give the user a broad overview of the input data, hopefully assisting in identifying corrupted files or aborted runs in the MRI.

Prerequisites: None.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{check\_spectra}
Usage: \texttt{check\_spectra <path> <expt> <mode> <uid>}

\begin{blockquote}
path -- full path to EPItome data folder.
expt -- experiment name.
mode -- image modality (eg., TASK, REST).
uid -- unique identifier for run of EPItome.
\end{blockquote}

\noindent This gives an overview of the frequency content of the MRI data in multiple ways for each subject. First, it plots the log-log spectra of the regressors used for time-series filtering, as typically done in resting-state experiments. It also compares the mean raw data with the mean filtered output, and mean noise model, to show whether the modeled noise is qualitatively different from the input raw data. Finally, it compares the spectra of the mean time series with the mean of all computed spectra, which should be equivalent.

Prerequisites: \texttt{linreg\_FS2EPI\_AFNI/FSL, gen\_regressors}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Cleanup}

These programs are run separately using \texttt{EPItome clean}. They generally provide the ability to eliminate faulty outputs or intermediate files from experiments. Due to their destructive nature, these scripts must be executed by hand and each step must be manually confirmed. They are therefore not amiable to unattended scripting, although one could easily write their own with some know-how. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{clean\_params}
Usage: \texttt{clean\_params}

\noindent If any error occurs while running the pipeline, faulty PARAMS file will be output (typically in 1D format). The presence of these files will confuse future runs of the pipeline, or downstream modules, resulting in cryptic errors. This module ONLY deletes PARAM files with no content, allowing you to flush your experiment of faulty outputs to permit a re-run of the pipeline.

Prerequisites: \texttt{clean\_params}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{del\_MNI}
Usage: \texttt{del\_MNI}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{del\_everything}
Usage: \texttt{del\_everything}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{del\_postmc}
Usage: \texttt{del\_postmc}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{del\_reg}
Usage: \texttt{del\_reg}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Workflows, a collection of presets
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Workflows}

EPItome give you the ability to chain modular BASH scripts together to generate a great number of MRI pre-processing pathways, and in doing so, gives you the power to create very bad pipelines. Here, I detail a few reasonable workflows.

\subsection{Basic MRI: GLM, PLS, etc.}

Here, we are interested in doing a set of basic tasks before running a GLM analysis on some task-based MRI design. We've already placed the anatomical and functional NIFTI (and .phys files, if appropriate) into their RUN folders and have run \texttt{EPItome run}. Every run begins with \texttt{init\_EPI} \\

\noindent
\texttt{init\_EPI high 0 alt+z off normal} \\

Here, we are telling EPItome that we are working with high contrast data, want to remove 0 TRs from the beginning of each run, have acquired our data in the alternating plus direction, we are turning off time series normalization, and would like a brain mask of normal tightness, which is a reasonable default. \\

\noindent
\texttt{linreg\_calc\_AFNI high lpc giant\_move} \\

Here we are calculating all of our registration pathways, from EPI space, to single-subject T1 space (and therefore, Freesurfer space), and finally group-level MNI space, using linear registrations. Some reference images and transformation pathways are output, but we haven't actually moved the data yet. \\

\noindent
\texttt{linreg\_FS2EPI\_AFNI} \\

This will put all of our Freesurfer-derived segmentations in single-subject EPI space, which we can use to generate regressors. \\

\noindent
\texttt{gen\_regressors scaled} \\

This will generate a set of regressors from the aforementioned Freesurfer atlases and the input data \texttt{scaled}, which was generated by \texttt{init\_EPI}. In this case, these regressors will be applied during the GLM as baselines. Note that it will always generate all regressors, but they need not all be used. \\

\noindent
\texttt{volsmooth scaled EPI\_mask 6.0} \\

This will smooth the EPI data within the defined mask (\texttt{anat\_EPI\_mask.nii.gz} in this case) using a full-width half-maximum (FWHM) of 6 mm. The input data is still scaled, since this is the first actual manipulation of the outputs from \texttt{init\_EPI}. \\

\noindent
\texttt{linreg\_EPI2MNI\_AFNI volsmooth 3.0} \\

Finally, this will transform each smoothed run up into MNI space with a isotropic voxel resolution of 3 mm$^2$. \\


\subsection{Connectivity Analysis}

Functional connectivity analysis benefits from the application of tissue-based regressors pre-analysis, unlike in a GLM where one can inset these regressors at the same time. For this reason, connectivity analysis will require a slightly modified pipeline: \\

\noindent
\texttt{init\_EPI high 0 alt+z off normal} \\
\texttt{linreg\_calc\_AFNI high lpc giant\_move} \\
\texttt{linreg\_FS2EPI\_AFNI} \\
\texttt{gen\_regressors scaled} \\

\noindent
\texttt{filter scaled 4 off on on on on} \\

Here, we are detrending the scaled data against the head motion parameters, Legendre polynomials up to the 4th order, the mean white matter signal, the local white matter signal, the mean cerebral spinal fluid signal, and the mean draining vessel signal. For all of these signals, we also regress against the 1st temporal lag. \\

\noindent
\texttt{lowpass filtered EPI\_mask average 3} \\

Next, we low-pass the data using a moving-average filter of span 3. Most of the information in BOLD data is of fairly low frequency, so the hope is that low-passing the data will remove some high-frequency noise from the signals. There are multiple options that could be used here, but no one ever got fired for using a moving average filter, so I suggest it here as a fair default.\\

\noindent
\texttt{volsmooth lowpass EPI\_mask 6.0} \\
\texttt{linreg\_EPI2MNI\_AFNI volsmooth 3.0} \\

\subsection{Surface Analysis \& Surface Smoothing for Volume Analysis}

When studying the cortex, it is often best to look at the data on a surface. This prevents the blurring of signals between sulci and gyri, allows for finer localization of function, and permits some interesting co-registration methods. For simplicity, we will do this to data intended for a simple GLM analysis. \\

\noindent
\texttt{init\_EPI high 0 alt+z off normal} \\
\texttt{linreg\_calc\_AFNI high lpc giant\_move} \\
\texttt{linreg\_FS2EPI\_AFNI} \\
\texttt{gen\_regressors scaled} \\

\noindent
\texttt{vol2surf scaled} \\

This will projects the EPI data contained within the white-matter boundaries of the Freesurfer segmentation to a AFNI-based surface space. This must be run on EPI data in single-subject T1 space, otherwise we won't end up projecting the cortex to the surface model, but rather some random selection of brain and non-brain matter! \\

\noindent
\texttt{surfsmooth surface 10.0} \\

This will smooth along the cortical surface with a FWHM of 10mm. Generally, surface-smoothed data can be subjected to larger smoothing kernels, as they do not mix signals coming from cortically-distant regions as readily in this format. \\

\noindent
\texttt{surf2vol smooth scaled} \\

This will project the surface data in smooth back into volume format in the same space as the scaled data, from whence it came. Many of the spatial-specificity advantages of surface-based analysis are now available in volume space, ensuring compatibility with many traditional analysis programs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% References, both on server building and MRI analysis
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Module Creation}

EPItome, as it stands, has very few novel features over traditional pipelining programs. However, its strength lies with ease of extensibility. Here, I will detail how one would create a new module to be included in the EPItome pipeline.

\subsection{Writing a Module}

Modules take the form of either BASH scripts, or stand alone programs (such is the case with most QC modules at the moment) with a few stylistic conventions. They are `active' so long as they are kept in a \texttt{.../epitome/modules/XXX} directory, and will be accessed by the pipeline according to their type. `freesurfer' and `pre' modules are accessed first by \texttt{EPItome run}, followed by those in `qc'. At the moment, the two freesurfer modules are not optional.

The modules themselves use a \href{http://tldp.org/LDP/abs/html/here-docs.html}{here-doc trick} to set variables defined on the command line first, and then \texttt{cat} the remaining script to \texttt{STDOUT}. Therefore, running a properly formatted module should not run anything, but should simply print it's contents out to the command line. There are a few reserved variables used in most, if not all, modules. 

\begin{itemize}
    \item{DIR\_SESS: A listing of all the sessions within a image modality.}
    \item{SESS: A variable denoting the current session.}
    \item{DIR\_RUNS: A listing of all the runs within a session.}
    \item{RUN: A variable denoting the current run.}
    \item{NUM: The run number.}
    \item{ID: The unique identifier of a EPItome run, defined globally.}
\end{itemize}

A module will typically loop through sessions, and then runs, taking an input file prefix (such as \texttt{func\_scaled}), performing a number of operations on that file (producing intermediate files worth keeping, or in other cases, temporary files that will be removed by the module's end), and sometimes outputting a single functional file with a new prefix (such as \texttt{func\_lowpass}). The anatomy of a call to an output file follows the convention \\

\texttt{filename.ID.NUM.extension}. \\

For NIFTI files, \texttt{filename} is typically \texttt{func\_prefix} or \texttt{anat\_prefix}, for 4D and 3D files, respectively. Regressors, QC metrics, and other parameter files are typically stored in a special \texttt{PARAMS} folder. Registrations are stored with the \texttt{reg\_X\_to\_Y} convention, and the extension appropriate to the program that generated them (be it AFNI or FSL).

A well-written module will never try to do anything that has already been done. Therefore, blocks of code are wrapped in an \\

\texttt{if [ -f filename.ID.NUM.extension ]; then; commands; fi} \\

\noindent 
loop. This is not mandatory, but highly recommended. It allows one to re-run the pipeline with a few tweaks, and the code will only act on files missing from the output structure. 

Finally, variables can be defined within the module to allow the user to set them before running the module via the command line. Each command-line argument should correspond to a variable at the top of the module, which is then referenced in the appropriate locations throughout the script. Since the variables are defined before each module, the name-space between modules does not need to be maintained. However, for consistency, it is best to select variable names that are specific and unlikely to have shared meanings in other areas of the pipeline. 

\subsection{Python Wrapper}

With modules written, an advanced user could write a master BASH script by hand to make use of it. However, most users will want to make use of the python-based command line interface, which will require you to write a small wrapper function in \texttt{.../epitome/commands.py}.

Each function should have the same name as the associated module, and accept a single variable \texttt{input\_name}. This denotes the filename prefix that the module will operate on.

Next, you should define the output prefix (e.g., \texttt{lowpass}). This will be passed on to the next module, assuming the user does not make any errors inputing the various module options.

Finally, the function should ask the user a single question for each command-line argument. I have built 4 `selector' functions for integers, floats, lists, and dictionaries. These final two allow the user to select from a set of options, with or without an accompanying description. The first two allow the use to input a numerical value for appropriate settings, for example, smoothing kernel size in millimeters.

This set of questions should be wrapped in a try-except loop, checking for ValueErrors. If the user inputs an inappropriate option, the function will throw an error and return the special type `None', which will prompt EPItome to ignore the current call to the function and ask the user to try again. If all is well, the collected variables should be passed to the \texttt{line} variable, which contains a BASH formatted string that will be printed to the master script.

The following is a code block demonstrating this structure (from the function \texttt{surfsmooth}).

\begin{lstlisting}
def surfsmooth(input_name):    
    output = 'smooth'

    print('\nSmoothing functional data on a cortical surface.')

    try:
        print('\nInput smoothing kernel FWHM (mm):')
        fwhm, output = selector_float(output)

    except ValueError as ve:
        return '', None

    line = ('. ${DIR_PIPE}/epitome/modules/pre/surfsmooth ' +
                                      str(input_name) + ' ' +
                                      str(fwhm))
    return line, output
\end{lstlisting}

\subsection{Documentation}

This is a very important part of module-building, and currently takes two forms: the \LaTeX document that produced this PDF, and the .JSON-formatted file \texttt{.../epitome/help.json} which should contain a similar set of information, around the command-line usage of your module. Remember -- EPItome modules should always be useful to advanced users who simply want to write their own master BASH script, and therefore, the documentation should contain enough information so that they can perform this task manually. Hopefully the current set of documentation is a sufficient guide for future development.

%% End of document. Go to reference list.
\newpage 
\bibliographystyle{apalike}
\bibliography{harford}
\end{document}
